{"cells":[{"cell_type":"markdown","source":["# CNN evaluation for cifar10\n"],"metadata":{"id":"_caibiu5blWe"}},{"cell_type":"code","source":["#import adahessian\n","!pip install torch_optimizer\n","import torch_optimizer as ada_optim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tgCrRqHGbrZO","outputId":"028adf79-63a0-4943-8d5e-71ed0b5980fd","executionInfo":{"status":"ok","timestamp":1717468513696,"user_tz":180,"elapsed":71727,"user":{"displayName":"Henrique F","userId":"00712228040511115186"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_optimizer\n","  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from torch_optimizer) (2.3.0+cu121)\n","Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n","  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (4.12.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.5.0->torch_optimizer)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.5.0->torch_optimizer)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.5.0->torch_optimizer)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.5.0->torch_optimizer)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.5.0->torch_optimizer)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.5.0->torch_optimizer)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.5.0->torch_optimizer)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.5.0->torch_optimizer)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.5.0->torch_optimizer)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.5.0->torch_optimizer)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.5.0->torch_optimizer)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.0->torch_optimizer) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.5.0->torch_optimizer)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.0->torch_optimizer) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-ranger, torch_optimizer\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ACpEMzYL2vh-","outputId":"1907f6a3-4aa2-4fba-836d-4d6ace60050e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:03<00:00, 47417205.88it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import os\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device\n","\n","# Carregar e Pré-processar o CIFAR-10\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n","                                         shuffle=False, num_workers=2)\n","\n","# Definir o Modelo\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False),\n","            nn.ReLU()\n","        )\n","\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Dropout(0.25)\n","        )\n","\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(64 * 16 * 16, 128, bias=False),\n","            nn.ReLU(),\n","            nn.Dropout(0.5)\n","        )\n","\n","        self.out = nn.Linear(128, 10, bias=False)\n","\n","        self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = x.view(x.size(0), -1)  # Flatten the output\n","        x = self.fc1(x)\n","        output = self.out(x)\n","        return output\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n","                nn.init.kaiming_normal_(m.weight)\n","            if isinstance(m, nn.Linear):\n","                nn.init.constant_(m.weight, 1e-4)  # Regularization term\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D0mf5mhI2Wla","outputId":"86b53585-4eca-4468-ed37-4e6628ea2efa"},"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleCNN(\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): ReLU()\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Dropout(p=0.25, inplace=False)\n","  )\n","  (fc1): Sequential(\n","    (0): Linear(in_features=16384, out_features=128, bias=False)\n","    (1): ReLU()\n","    (2): Dropout(p=0.5, inplace=False)\n","  )\n","  (out): Linear(in_features=128, out_features=10, bias=False)\n",")\n"]}],"source":["# Instanciar o modelo\n","model = SimpleCNN().to(device)\n","print(model)\n","\n","# Definir a função de perda\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Treinar o modelo\n","def train(model, trainloader, testloader, criterion, optimizer, epochs=10):\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        #training\n","        model.train()\n","        train_loss = 0.0\n","        for inputs, labels in trainloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","            train_loss += loss.item()*inputs.size(0)\n","        train_loss = train_loss/len(trainloader.dataset)\n","        train_losses.append(train_loss)\n","\n","        #validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in testloader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                # Forward pass\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate accuracy\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_loss / len(testloader.dataset)\n","        val_losses.append(val_loss)\n","        print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.2f}%'\n","              .format(epoch+1, epochs, train_loss, val_loss, 100 * correct / total))\n","\n","\n","\n","\n","\n","    print(\"Finished Training/Validation\")\n","    return train_losses.copy(), val_losses.copy()\n","\n","#train(model, trainloader, testloader, criterion, optimizer, epochs=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1nGzkIM2eRa","outputId":"0224c81e-216a-4c14-a088-a4f94355dbe9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the network on the 10000 test images: 10.0 %\n","0.1\n"]}],"source":["# Avaliar o modelo\n","def test(model, testloader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in testloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n","    acc = correct/total\n","    return(acc)\n","\n","print(test(model, testloader))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kk6nr0PW7xsW","outputId":"b2a40348-b2ca-4c2c-e22b-8eb26fb52750"},"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleCNN(\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): ReLU()\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Dropout(p=0.25, inplace=False)\n","  )\n","  (fc1): Sequential(\n","    (0): Linear(in_features=16384, out_features=128, bias=False)\n","    (1): ReLU()\n","    (2): Dropout(p=0.5, inplace=False)\n","  )\n","  (out): Linear(in_features=128, out_features=10, bias=False)\n",")\n","Epoch [1/50], Train Loss: 1.5902, Val Loss: 1.2271, Val Acc: 56.81%\n","Epoch [2/50], Train Loss: 1.2520, Val Loss: 1.0465, Val Acc: 63.11%\n","Epoch [3/50], Train Loss: 1.1086, Val Loss: 0.9864, Val Acc: 66.12%\n","Epoch [4/50], Train Loss: 1.0127, Val Loss: 0.9676, Val Acc: 66.24%\n","Epoch [5/50], Train Loss: 0.9359, Val Loss: 0.9148, Val Acc: 68.32%\n","Epoch [6/50], Train Loss: 0.8741, Val Loss: 0.8994, Val Acc: 69.16%\n","Epoch [7/50], Train Loss: 0.8110, Val Loss: 0.8975, Val Acc: 69.50%\n","Epoch [8/50], Train Loss: 0.7698, Val Loss: 0.8985, Val Acc: 69.75%\n","Epoch [9/50], Train Loss: 0.7366, Val Loss: 0.9013, Val Acc: 69.89%\n","Epoch [10/50], Train Loss: 0.6949, Val Loss: 0.9247, Val Acc: 70.05%\n","Epoch [11/50], Train Loss: 0.6649, Val Loss: 0.9161, Val Acc: 70.18%\n","Epoch [12/50], Train Loss: 0.6373, Val Loss: 0.9373, Val Acc: 69.59%\n","Epoch [13/50], Train Loss: 0.6051, Val Loss: 0.9458, Val Acc: 70.40%\n","Epoch [14/50], Train Loss: 0.5887, Val Loss: 0.9590, Val Acc: 70.46%\n","Epoch [15/50], Train Loss: 0.5792, Val Loss: 0.9475, Val Acc: 70.12%\n","Epoch [16/50], Train Loss: 0.5535, Val Loss: 0.9982, Val Acc: 70.51%\n","Epoch [17/50], Train Loss: 0.5390, Val Loss: 0.9849, Val Acc: 70.29%\n","Epoch [18/50], Train Loss: 0.5217, Val Loss: 1.0030, Val Acc: 70.06%\n","Epoch [19/50], Train Loss: 0.5074, Val Loss: 1.0111, Val Acc: 70.68%\n","Epoch [20/50], Train Loss: 0.4936, Val Loss: 1.0174, Val Acc: 70.54%\n","Epoch [21/50], Train Loss: 0.4879, Val Loss: 1.0555, Val Acc: 70.08%\n","Epoch [22/50], Train Loss: 0.4752, Val Loss: 1.0515, Val Acc: 70.33%\n","Epoch [23/50], Train Loss: 0.4720, Val Loss: 1.0575, Val Acc: 70.77%\n","Epoch [24/50], Train Loss: 0.4586, Val Loss: 1.0714, Val Acc: 70.77%\n","Epoch [25/50], Train Loss: 0.4520, Val Loss: 1.0620, Val Acc: 70.98%\n","Epoch [26/50], Train Loss: 0.4461, Val Loss: 1.0796, Val Acc: 70.61%\n","Epoch [27/50], Train Loss: 0.4411, Val Loss: 1.0517, Val Acc: 70.63%\n","Epoch [28/50], Train Loss: 0.4304, Val Loss: 1.1280, Val Acc: 70.28%\n","Epoch [29/50], Train Loss: 0.4291, Val Loss: 1.0957, Val Acc: 70.66%\n","Epoch [30/50], Train Loss: 0.4223, Val Loss: 1.1142, Val Acc: 70.88%\n","Epoch [31/50], Train Loss: 0.4136, Val Loss: 1.0961, Val Acc: 70.94%\n","Epoch [32/50], Train Loss: 0.4084, Val Loss: 1.1025, Val Acc: 70.92%\n","Epoch [33/50], Train Loss: 0.4035, Val Loss: 1.1257, Val Acc: 70.75%\n","Epoch [34/50], Train Loss: 0.4080, Val Loss: 1.1650, Val Acc: 70.56%\n","Epoch [35/50], Train Loss: 0.3955, Val Loss: 1.1350, Val Acc: 70.72%\n","Epoch [36/50], Train Loss: 0.3939, Val Loss: 1.1799, Val Acc: 70.85%\n","Epoch [37/50], Train Loss: 0.3892, Val Loss: 1.1559, Val Acc: 71.04%\n","Epoch [38/50], Train Loss: 0.3843, Val Loss: 1.1822, Val Acc: 70.33%\n","Epoch [39/50], Train Loss: 0.3776, Val Loss: 1.1605, Val Acc: 71.03%\n","Epoch [40/50], Train Loss: 0.3766, Val Loss: 1.2215, Val Acc: 70.98%\n","Epoch [41/50], Train Loss: 0.3776, Val Loss: 1.2024, Val Acc: 70.68%\n","Epoch [42/50], Train Loss: 0.3665, Val Loss: 1.1827, Val Acc: 70.95%\n","Epoch [43/50], Train Loss: 0.3629, Val Loss: 1.1832, Val Acc: 70.97%\n","Epoch [44/50], Train Loss: 0.3589, Val Loss: 1.1718, Val Acc: 70.59%\n","Epoch [45/50], Train Loss: 0.3612, Val Loss: 1.1805, Val Acc: 70.70%\n","Epoch [46/50], Train Loss: 0.3476, Val Loss: 1.2217, Val Acc: 70.74%\n","Epoch [47/50], Train Loss: 0.3566, Val Loss: 1.2111, Val Acc: 70.99%\n","Epoch [48/50], Train Loss: 0.3518, Val Loss: 1.2187, Val Acc: 70.49%\n","Epoch [49/50], Train Loss: 0.3445, Val Loss: 1.2200, Val Acc: 70.62%\n","Epoch [50/50], Train Loss: 0.3473, Val Loss: 1.2353, Val Acc: 71.18%\n","Finished Training/Validation\n","Accuracy of the network on the 10000 test images: 71.18 %\n"]}],"source":["results={}\n","\n","\n","#ADAM\n","# Instanciar o modelo\n","model = SimpleCNN().to(device)\n","print(model)\n","\n","# Definir a função de perda\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","\n","\n","\n","model_perform={}\n","\n","#treina o modelo com 30 epocas\n","model_perform['train_losses'], model_perform['validation_losses'] = train(model, trainloader, testloader, criterion, optimizer, epochs=50)\n","model_perform['accuracy'] = test(model, testloader)\n","results['adam'] = model_perform\n","torch.save(model.state_dict(),'adam_cnn_cifa10')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_LaVD2UPK1RX","outputId":"41844298-3ada-42c2-d7d7-1eb26f52ed48"},"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleCNN(\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): ReLU()\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Dropout(p=0.25, inplace=False)\n","  )\n","  (fc1): Sequential(\n","    (0): Linear(in_features=16384, out_features=128, bias=False)\n","    (1): ReLU()\n","    (2): Dropout(p=0.5, inplace=False)\n","  )\n","  (out): Linear(in_features=128, out_features=10, bias=False)\n",")\n","Epoch [1/50], Train Loss: 2.3001, Val Loss: 2.2985, Val Acc: 12.29%\n","Epoch [2/50], Train Loss: 2.2870, Val Loss: 2.2510, Val Acc: 15.89%\n","Epoch [3/50], Train Loss: 2.1694, Val Loss: 2.0897, Val Acc: 21.98%\n","Epoch [4/50], Train Loss: 2.0550, Val Loss: 1.9961, Val Acc: 26.85%\n","Epoch [5/50], Train Loss: 1.9803, Val Loss: 1.9258, Val Acc: 30.16%\n","Epoch [6/50], Train Loss: 1.9183, Val Loss: 1.8657, Val Acc: 32.69%\n","Epoch [7/50], Train Loss: 1.8670, Val Loss: 1.8136, Val Acc: 34.62%\n","Epoch [8/50], Train Loss: 1.8249, Val Loss: 1.7708, Val Acc: 36.43%\n","Epoch [9/50], Train Loss: 1.7849, Val Loss: 1.7276, Val Acc: 37.78%\n","Epoch [10/50], Train Loss: 1.7459, Val Loss: 1.6883, Val Acc: 39.51%\n","Epoch [11/50], Train Loss: 1.7100, Val Loss: 1.6498, Val Acc: 40.88%\n","Epoch [12/50], Train Loss: 1.6759, Val Loss: 1.6124, Val Acc: 42.24%\n","Epoch [13/50], Train Loss: 1.6452, Val Loss: 1.5864, Val Acc: 42.74%\n","Epoch [14/50], Train Loss: 1.6141, Val Loss: 1.5521, Val Acc: 44.44%\n","Epoch [15/50], Train Loss: 1.5907, Val Loss: 1.5268, Val Acc: 45.11%\n","Epoch [16/50], Train Loss: 1.5659, Val Loss: 1.4985, Val Acc: 46.13%\n","Epoch [17/50], Train Loss: 1.5431, Val Loss: 1.4760, Val Acc: 47.17%\n","Epoch [18/50], Train Loss: 1.5226, Val Loss: 1.4567, Val Acc: 47.81%\n","Epoch [19/50], Train Loss: 1.4989, Val Loss: 1.4380, Val Acc: 48.36%\n","Epoch [20/50], Train Loss: 1.4817, Val Loss: 1.4202, Val Acc: 49.31%\n","Epoch [21/50], Train Loss: 1.4619, Val Loss: 1.4127, Val Acc: 49.15%\n","Epoch [22/50], Train Loss: 1.4437, Val Loss: 1.3907, Val Acc: 50.18%\n","Epoch [23/50], Train Loss: 1.4286, Val Loss: 1.3715, Val Acc: 50.97%\n","Epoch [24/50], Train Loss: 1.4161, Val Loss: 1.3587, Val Acc: 51.64%\n","Epoch [25/50], Train Loss: 1.3982, Val Loss: 1.3459, Val Acc: 52.20%\n","Epoch [26/50], Train Loss: 1.3872, Val Loss: 1.3296, Val Acc: 53.02%\n","Epoch [27/50], Train Loss: 1.3760, Val Loss: 1.3197, Val Acc: 53.82%\n","Epoch [28/50], Train Loss: 1.3609, Val Loss: 1.3069, Val Acc: 53.77%\n","Epoch [29/50], Train Loss: 1.3469, Val Loss: 1.2974, Val Acc: 54.52%\n","Epoch [30/50], Train Loss: 1.3345, Val Loss: 1.2928, Val Acc: 54.32%\n","Epoch [31/50], Train Loss: 1.3232, Val Loss: 1.2789, Val Acc: 55.54%\n","Epoch [32/50], Train Loss: 1.3156, Val Loss: 1.2679, Val Acc: 55.81%\n","Epoch [33/50], Train Loss: 1.3029, Val Loss: 1.2664, Val Acc: 55.63%\n","Epoch [34/50], Train Loss: 1.2943, Val Loss: 1.2519, Val Acc: 55.59%\n","Epoch [35/50], Train Loss: 1.2805, Val Loss: 1.2446, Val Acc: 56.26%\n","Epoch [36/50], Train Loss: 1.2736, Val Loss: 1.2442, Val Acc: 55.68%\n","Epoch [37/50], Train Loss: 1.2648, Val Loss: 1.2279, Val Acc: 56.56%\n","Epoch [38/50], Train Loss: 1.2532, Val Loss: 1.2236, Val Acc: 56.81%\n","Epoch [39/50], Train Loss: 1.2416, Val Loss: 1.2122, Val Acc: 57.43%\n","Epoch [40/50], Train Loss: 1.2358, Val Loss: 1.2016, Val Acc: 57.51%\n","Epoch [41/50], Train Loss: 1.2261, Val Loss: 1.2074, Val Acc: 57.19%\n","Epoch [42/50], Train Loss: 1.2151, Val Loss: 1.1910, Val Acc: 57.72%\n","Epoch [43/50], Train Loss: 1.2073, Val Loss: 1.1892, Val Acc: 58.35%\n","Epoch [44/50], Train Loss: 1.1996, Val Loss: 1.1713, Val Acc: 58.61%\n","Epoch [45/50], Train Loss: 1.1915, Val Loss: 1.1683, Val Acc: 58.70%\n","Epoch [46/50], Train Loss: 1.1800, Val Loss: 1.1639, Val Acc: 59.06%\n","Epoch [47/50], Train Loss: 1.1736, Val Loss: 1.1573, Val Acc: 59.14%\n","Epoch [48/50], Train Loss: 1.1604, Val Loss: 1.1502, Val Acc: 59.67%\n","Epoch [49/50], Train Loss: 1.1536, Val Loss: 1.1445, Val Acc: 59.98%\n","Epoch [50/50], Train Loss: 1.1463, Val Loss: 1.1310, Val Acc: 60.37%\n","Finished Training/Validation\n","Accuracy of the network on the 10000 test images: 60.37 %\n"]}],"source":["#SGD\n","\n","# Instanciar o modelo\n","model = SimpleCNN().to(device)\n","print(model)\n","\n","# Definir a função de perda\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001)\n","\n","\n","\n","model_perform={}\n","#treina o modelo com 30 epocas\n","model_perform['train_losses'], model_perform['validation_losses'] = train(model, trainloader, testloader, criterion, optimizer, epochs=50)\n","model_perform['accuracy'] = test(model, testloader)\n","results['sgd'] = model_perform\n","torch.save(model.state_dict(),'sgd_cnn_cifar10')"]},{"cell_type":"code","source":["# Treinar o modelo\n","def train_hess(model, trainloader, testloader, criterion, optimizer, epochs=10):\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        #training\n","        model.train()\n","        train_loss = 0.0\n","        for inputs, labels in trainloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward(create_graph=True)\n","            optimizer.step()\n","\n","\n","            train_loss += loss.item()*inputs.size(0)\n","        train_loss = train_loss/len(trainloader.dataset)\n","        train_losses.append(train_loss)\n","\n","        #validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in testloader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                # Forward pass\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate accuracy\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_loss / len(testloader.dataset)\n","        val_losses.append(val_loss)\n","        print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.2f}%'\n","              .format(epoch+1, epochs, train_loss, val_loss, 100 * correct / total))\n","\n","\n","\n","\n","\n","    print(\"Finished Training/Validation\")\n","    return train_losses.copy(), val_losses.copy()\n","\n","\n","#ADAHESSIAN\n","\n","# Instanciar o modelo\n","model = SimpleCNN().to(device)\n","print(model)\n","\n","# Definir a função de perda\n","criterion = nn.CrossEntropyLoss()\n","optimizer = ada_optim.Adahessian(model.parameters(), lr = 0.001)\n","\n","\n","\n","model_perform={}\n","#treina o modelo com 30 epocas\n","model_perform['train_losses'], model_perform['validation_losses'] = train_hess(model, trainloader, testloader, criterion, optimizer, epochs=50)\n","model_perform['accuracy'] = test(model, testloader)\n","results['Adahessian'] = model_perform\n","torch.save(model.state_dict(),'adahessian_cnn_cifar10')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNZejq2ibSGm","outputId":"3eb53920-3bfa-413f-c40e-f25bdd72425b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleCNN(\n","  (conv1): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): ReLU()\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Dropout(p=0.25, inplace=False)\n","  )\n","  (fc1): Sequential(\n","    (0): Linear(in_features=16384, out_features=128, bias=False)\n","    (1): ReLU()\n","    (2): Dropout(p=0.5, inplace=False)\n","  )\n","  (out): Linear(in_features=128, out_features=10, bias=False)\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1203.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/50], Train Loss: 2.3003, Val Loss: 2.2981, Val Acc: 10.00%\n","Epoch [2/50], Train Loss: 2.2901, Val Loss: 2.2673, Val Acc: 15.79%\n","Epoch [3/50], Train Loss: 2.1889, Val Loss: 2.0961, Val Acc: 20.89%\n","Epoch [4/50], Train Loss: 2.0516, Val Loss: 1.9843, Val Acc: 26.24%\n","Epoch [5/50], Train Loss: 1.9670, Val Loss: 1.9058, Val Acc: 31.75%\n","Epoch [6/50], Train Loss: 1.9025, Val Loss: 1.8402, Val Acc: 34.61%\n","Epoch [7/50], Train Loss: 1.8449, Val Loss: 1.7798, Val Acc: 36.58%\n","Epoch [8/50], Train Loss: 1.7925, Val Loss: 1.7251, Val Acc: 38.61%\n","Epoch [9/50], Train Loss: 1.7433, Val Loss: 1.6740, Val Acc: 40.11%\n","Epoch [10/50], Train Loss: 1.6972, Val Loss: 1.6275, Val Acc: 41.68%\n","Epoch [11/50], Train Loss: 1.6578, Val Loss: 1.5875, Val Acc: 42.97%\n","Epoch [12/50], Train Loss: 1.6197, Val Loss: 1.5555, Val Acc: 44.40%\n","Epoch [13/50], Train Loss: 1.5902, Val Loss: 1.5254, Val Acc: 45.29%\n","Epoch [14/50], Train Loss: 1.5609, Val Loss: 1.4962, Val Acc: 46.00%\n","Epoch [15/50], Train Loss: 1.5355, Val Loss: 1.4671, Val Acc: 47.14%\n","Epoch [16/50], Train Loss: 1.5118, Val Loss: 1.4481, Val Acc: 48.11%\n","Epoch [17/50], Train Loss: 1.4924, Val Loss: 1.4265, Val Acc: 48.74%\n","Epoch [18/50], Train Loss: 1.4670, Val Loss: 1.4097, Val Acc: 49.61%\n","Epoch [19/50], Train Loss: 1.4500, Val Loss: 1.3926, Val Acc: 50.15%\n","Epoch [20/50], Train Loss: 1.4356, Val Loss: 1.3794, Val Acc: 50.84%\n","Epoch [21/50], Train Loss: 1.4208, Val Loss: 1.3646, Val Acc: 51.20%\n","Epoch [22/50], Train Loss: 1.4048, Val Loss: 1.3454, Val Acc: 52.09%\n","Epoch [23/50], Train Loss: 1.3890, Val Loss: 1.3357, Val Acc: 52.48%\n","Epoch [24/50], Train Loss: 1.3759, Val Loss: 1.3284, Val Acc: 53.02%\n","Epoch [25/50], Train Loss: 1.3678, Val Loss: 1.3149, Val Acc: 53.30%\n","Epoch [26/50], Train Loss: 1.3529, Val Loss: 1.3062, Val Acc: 53.73%\n","Epoch [27/50], Train Loss: 1.3393, Val Loss: 1.2946, Val Acc: 54.31%\n","Epoch [28/50], Train Loss: 1.3268, Val Loss: 1.2862, Val Acc: 54.68%\n","Epoch [29/50], Train Loss: 1.3163, Val Loss: 1.2778, Val Acc: 54.96%\n","Epoch [30/50], Train Loss: 1.3079, Val Loss: 1.2641, Val Acc: 55.52%\n","Epoch [31/50], Train Loss: 1.2993, Val Loss: 1.2572, Val Acc: 55.70%\n","Epoch [32/50], Train Loss: 1.2867, Val Loss: 1.2514, Val Acc: 55.67%\n","Epoch [33/50], Train Loss: 1.2806, Val Loss: 1.2435, Val Acc: 55.79%\n","Epoch [34/50], Train Loss: 1.2667, Val Loss: 1.2338, Val Acc: 56.69%\n","Epoch [35/50], Train Loss: 1.2602, Val Loss: 1.2250, Val Acc: 56.58%\n","Epoch [36/50], Train Loss: 1.2492, Val Loss: 1.2222, Val Acc: 57.03%\n","Epoch [37/50], Train Loss: 1.2403, Val Loss: 1.2113, Val Acc: 57.16%\n","Epoch [38/50], Train Loss: 1.2329, Val Loss: 1.2058, Val Acc: 57.31%\n","Epoch [39/50], Train Loss: 1.2235, Val Loss: 1.2035, Val Acc: 57.33%\n","Epoch [40/50], Train Loss: 1.2160, Val Loss: 1.1936, Val Acc: 58.34%\n","Epoch [41/50], Train Loss: 1.2059, Val Loss: 1.1855, Val Acc: 58.01%\n","Epoch [42/50], Train Loss: 1.2000, Val Loss: 1.1796, Val Acc: 58.27%\n","Epoch [43/50], Train Loss: 1.1885, Val Loss: 1.1740, Val Acc: 58.83%\n","Epoch [44/50], Train Loss: 1.1838, Val Loss: 1.1676, Val Acc: 58.43%\n","Epoch [45/50], Train Loss: 1.1782, Val Loss: 1.1614, Val Acc: 58.91%\n","Epoch [46/50], Train Loss: 1.1665, Val Loss: 1.1542, Val Acc: 59.70%\n","Epoch [47/50], Train Loss: 1.1633, Val Loss: 1.1465, Val Acc: 59.63%\n","Epoch [48/50], Train Loss: 1.1525, Val Loss: 1.1434, Val Acc: 60.18%\n","Epoch [49/50], Train Loss: 1.1457, Val Loss: 1.1390, Val Acc: 59.98%\n","Epoch [50/50], Train Loss: 1.1412, Val Loss: 1.1380, Val Acc: 59.75%\n","Finished Training/Validation\n","Accuracy of the network on the 10000 test images: 59.75 %\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jn19yeC-L-TO","outputId":"5cc7fe73-c2bc-4e57-bc06-f56e474e9632"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'adam': {'train_losses': [1.590164584465027,\n","   1.2519942048645019,\n","   1.1086417794799805,\n","   1.0127390199279784,\n","   0.9358503803253174,\n","   0.874054369392395,\n","   0.8110197282791137,\n","   0.7698344395637512,\n","   0.7366180632781982,\n","   0.6949183436584473,\n","   0.6649042266654969,\n","   0.6373139685630799,\n","   0.605095704870224,\n","   0.5887161101341247,\n","   0.579220671710968,\n","   0.5535266841793061,\n","   0.5390034580230713,\n","   0.5216813322257996,\n","   0.5074019215869904,\n","   0.4936124308204651,\n","   0.4878903732442856,\n","   0.4752003056716919,\n","   0.4720305403518677,\n","   0.4585906790828705,\n","   0.45204107931137083,\n","   0.4461482344055176,\n","   0.441104617767334,\n","   0.4303910920333862,\n","   0.42905445552825927,\n","   0.4222751952934265,\n","   0.41356560861587527,\n","   0.4084376046562195,\n","   0.40346481088638303,\n","   0.4079769277381897,\n","   0.3955028570461273,\n","   0.3938916843032837,\n","   0.38916474439620974,\n","   0.3842560176563263,\n","   0.377643616733551,\n","   0.37661827850341795,\n","   0.37761816915512086,\n","   0.36651902200222014,\n","   0.362888324174881,\n","   0.35888387969970703,\n","   0.36117295870780947,\n","   0.34755015257716176,\n","   0.35658752440452574,\n","   0.35180156658172607,\n","   0.3445219389629364,\n","   0.347333965511322],\n","  'validation_losses': [1.227099472618103,\n","   1.046548403930664,\n","   0.9863689215660095,\n","   0.9676219882011413,\n","   0.9147918231964112,\n","   0.8994213315963745,\n","   0.8974945436477662,\n","   0.8985006801605224,\n","   0.9013138993263244,\n","   0.9246547576904297,\n","   0.9160942157745361,\n","   0.9372734529495239,\n","   0.9458094563484192,\n","   0.9590299390792847,\n","   0.9475139017105103,\n","   0.9981917413711547,\n","   0.9848899782180787,\n","   1.0030361734390258,\n","   1.0111342485427857,\n","   1.0173753704071045,\n","   1.0555309000015258,\n","   1.0515123723983764,\n","   1.057529187297821,\n","   1.0713635703086852,\n","   1.0619718173980712,\n","   1.0795713928222657,\n","   1.0517119848251342,\n","   1.1280404106140136,\n","   1.0957063459396363,\n","   1.1141604035377501,\n","   1.0960929152488708,\n","   1.1024824989318847,\n","   1.125656040763855,\n","   1.165009277534485,\n","   1.1349609920501709,\n","   1.1798676935195922,\n","   1.155896432495117,\n","   1.1822431922912597,\n","   1.160453106880188,\n","   1.2214729787826537,\n","   1.20244501953125,\n","   1.182720745277405,\n","   1.1831819469451905,\n","   1.1718389612197877,\n","   1.1804557359695436,\n","   1.2217176908493041,\n","   1.2111256501197816,\n","   1.2186912669181824,\n","   1.2199837715148927,\n","   1.2353064406394958],\n","  'accuracy': 0.7118},\n"," 'sgd': {'train_losses': [2.300112315826416,\n","   2.287028645095825,\n","   2.1693725386047364,\n","   2.0549666287231445,\n","   1.9803386457061767,\n","   1.9183007612609864,\n","   1.8669505683898926,\n","   1.824855620689392,\n","   1.7848964277648927,\n","   1.7459018146514893,\n","   1.7100326007461548,\n","   1.6759333223724364,\n","   1.6451503852844238,\n","   1.6140886610412597,\n","   1.59071629901886,\n","   1.5659311952209474,\n","   1.5430639961242676,\n","   1.5225665803146362,\n","   1.4988531211853027,\n","   1.481671733856201,\n","   1.46186568775177,\n","   1.4437493642425536,\n","   1.4286047667312622,\n","   1.41610542137146,\n","   1.398249234085083,\n","   1.3872130709075927,\n","   1.3760212105178833,\n","   1.3609212716674806,\n","   1.3469104176712037,\n","   1.3345414585876465,\n","   1.3232211966323852,\n","   1.3156102284240723,\n","   1.3028838509368896,\n","   1.2943405065727234,\n","   1.2804504856491088,\n","   1.2735601488876342,\n","   1.264847137336731,\n","   1.2531841342926024,\n","   1.241560542755127,\n","   1.2357665762710572,\n","   1.226130982208252,\n","   1.215082363319397,\n","   1.2072790614700317,\n","   1.1996310301208497,\n","   1.191494501914978,\n","   1.180049096469879,\n","   1.173617470779419,\n","   1.1604349911499023,\n","   1.1536044628143312,\n","   1.1462542750549316],\n","  'validation_losses': [2.29847631149292,\n","   2.251006341934204,\n","   2.0897078031539915,\n","   1.9960591348648071,\n","   1.9258005451202393,\n","   1.8657427843093872,\n","   1.813598589515686,\n","   1.7708096385955812,\n","   1.7276498233795166,\n","   1.688256097793579,\n","   1.649776728439331,\n","   1.6124353481292724,\n","   1.586416738319397,\n","   1.5521224773406983,\n","   1.5268417318344116,\n","   1.4985439378738403,\n","   1.4759899696350098,\n","   1.456707022857666,\n","   1.4379584686279296,\n","   1.4201522214889526,\n","   1.4127104598999023,\n","   1.3907027032852173,\n","   1.371453876876831,\n","   1.3587031358718873,\n","   1.3458926235198974,\n","   1.3295693994522095,\n","   1.319709196472168,\n","   1.3069010612487792,\n","   1.2973677116394042,\n","   1.2927793329238892,\n","   1.2788761455535889,\n","   1.2678990224838256,\n","   1.2663519254684448,\n","   1.2519177434921265,\n","   1.2445528856277466,\n","   1.244184276008606,\n","   1.2279290412902832,\n","   1.2235683195114135,\n","   1.2122313336372375,\n","   1.2015718313217163,\n","   1.2074287322044372,\n","   1.1909618771553039,\n","   1.189159972000122,\n","   1.1713412457466126,\n","   1.168328870677948,\n","   1.1639262349128723,\n","   1.1572616581916808,\n","   1.1501674314498902,\n","   1.1445322150230408,\n","   1.131002944946289],\n","  'accuracy': 0.6037},\n"," 'Adahessian': {'train_losses': [2.300267517852783,\n","   2.2901430268096923,\n","   2.188936166534424,\n","   2.0515802210998535,\n","   1.9670093835830689,\n","   1.9025015752792358,\n","   1.8449413418579101,\n","   1.7924989828491211,\n","   1.743339294052124,\n","   1.6971755513763427,\n","   1.6578333689117433,\n","   1.619663718109131,\n","   1.5902247226715087,\n","   1.5609484342193602,\n","   1.535524635734558,\n","   1.5118297251129151,\n","   1.492388031578064,\n","   1.4669682010269165,\n","   1.450033404045105,\n","   1.4355797248458861,\n","   1.4208147137451173,\n","   1.4047708878326417,\n","   1.3890334538269042,\n","   1.3759057288742065,\n","   1.367834640235901,\n","   1.3529050099945068,\n","   1.3392588707733155,\n","   1.326843090057373,\n","   1.3162619505691528,\n","   1.3079352531051636,\n","   1.2992861337280273,\n","   1.2867393689727784,\n","   1.2806072607421874,\n","   1.2667420092391968,\n","   1.2601730399703979,\n","   1.2491527196884156,\n","   1.2403311031723023,\n","   1.2329197974586488,\n","   1.223481057472229,\n","   1.2160291701126098,\n","   1.2059074445343017,\n","   1.2000035441589356,\n","   1.1885259299850464,\n","   1.1837628115463257,\n","   1.178230959224701,\n","   1.1665135195541383,\n","   1.1632775369262696,\n","   1.1524911684036254,\n","   1.145690873336792,\n","   1.1411736603736877],\n","  'validation_losses': [2.298073947906494,\n","   2.2673465732574463,\n","   2.096058676338196,\n","   1.9843070249557495,\n","   1.9058438570022582,\n","   1.8401756494522095,\n","   1.779812666130066,\n","   1.7250706546783448,\n","   1.6740129011154175,\n","   1.6274644578933717,\n","   1.5874844272613526,\n","   1.5555357265472411,\n","   1.5253797798156739,\n","   1.496161359024048,\n","   1.4671256172180176,\n","   1.4480896780014039,\n","   1.4265436275482177,\n","   1.4097286911010742,\n","   1.3925811359405518,\n","   1.3794375806808472,\n","   1.3645821367263793,\n","   1.3454200582504272,\n","   1.3357095083236694,\n","   1.3283641033172608,\n","   1.3148846437454225,\n","   1.306212505531311,\n","   1.2946447692871095,\n","   1.2862070526123046,\n","   1.2778018928527832,\n","   1.2641109489440918,\n","   1.2571891771316528,\n","   1.2514229200363158,\n","   1.2435430479049683,\n","   1.2337593896865844,\n","   1.22503998336792,\n","   1.2221579315185547,\n","   1.2112533910751342,\n","   1.2058019584655761,\n","   1.2035234855651855,\n","   1.193552389717102,\n","   1.1855140602111816,\n","   1.1795900499343872,\n","   1.174017657470703,\n","   1.1675673795700072,\n","   1.161424868106842,\n","   1.1541846549987793,\n","   1.146480128288269,\n","   1.1434023270606994,\n","   1.1389746168136596,\n","   1.137963313293457],\n","  'accuracy': 0.5975}}"]},"metadata":{},"execution_count":21}],"source":["import json\n","with open('results.json', 'w') as f:\n","    json.dump(results, f)\n","results"]},{"cell_type":"markdown","source":["# MLP evaluation for cifar10"],"metadata":{"id":"fpTiBQX-j-vF"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import os\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device\n","\n","# Carregar e Pré-processar o CIFAR-10\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n","                                         shuffle=False, num_workers=2)\n","\n","# Definir o Modelo\n","\n","class SimpleMLP(nn.Module):\n","    def __init__(self, input_size=3*32*32, num_classes=10):\n","        super(SimpleMLP, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(input_size, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.25)\n","        )\n","        self.out = nn.Sequential(\n","            nn.Linear(512, num_classes),\n","            nn.Softmax(dim=1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        output = self.out(x)\n","        return output\n","\n","# Treinar o modelo\n","def train(model, trainloader, testloader, criterion, optimizer, epochs=10):\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        #training\n","        model.train()\n","        train_loss = 0.0\n","        for inputs, labels in trainloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","            train_loss += loss.item()*inputs.size(0)\n","        train_loss = train_loss/len(trainloader.dataset)\n","        train_losses.append(train_loss)\n","\n","        #validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in testloader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                # Forward pass\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate accuracy\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_loss / len(testloader.dataset)\n","        val_losses.append(val_loss)\n","        print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.2f}%'\n","              .format(epoch+1, epochs, train_loss, val_loss, 100 * correct / total))\n","\n","\n","\n","\n","\n","    print(\"Finished Training/Validation\")\n","    return train_losses.copy(), val_losses.copy()\n","\n","# Avaliar o modelo\n","def test(model, testloader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for inputs, labels in testloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    print(f'Accuracy of the network on the 10000 test images: {100 * correct / total} %')\n","    acc = correct/total\n","    return(acc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7xUBVxGBkCx0","outputId":"511d73a9-7bf3-44d1-9e82-dbcf88564e19","executionInfo":{"status":"ok","timestamp":1717466610403,"user_tz":180,"elapsed":1719,"user":{"displayName":"Henrique F","userId":"00712228040511115186"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["results_mlp={}\n","\n","\n","#ADAM\n","# Instanciar o modelo\n","model = SimpleMLP().to(device)\n","print(model)\n","\n","# Definir a função de perda\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","\n","\n","\n","model_perform={}\n","\n","#treina o modelo com 30 epocas\n","model_perform['train_losses'], model_perform['validation_losses'] = train(model, trainloader, testloader, criterion, optimizer, epochs=50)\n","model_perform['accuracy'] = test(model, testloader)\n","results_mlp['adam'] = model_perform\n","torch.save(model.state_dict(),'adam_mlp_cifar10')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2CqZuGCUkel6","outputId":"50bd2fbf-65bb-4bdd-d295-27bad40e1a86","executionInfo":{"status":"ok","timestamp":1717468315375,"user_tz":180,"elapsed":836661,"user":{"displayName":"Henrique F","userId":"00712228040511115186"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleMLP(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (fc1): Sequential(\n","    (0): Linear(in_features=3072, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.25, inplace=False)\n","  )\n","  (out): Sequential(\n","    (0): Linear(in_features=512, out_features=10, bias=True)\n","    (1): Softmax(dim=1)\n","  )\n",")\n","Epoch [1/50], Train Loss: 2.1062, Val Loss: 2.0791, Val Acc: 37.93%\n","Epoch [2/50], Train Loss: 2.0769, Val Loss: 2.0557, Val Acc: 40.17%\n","Epoch [3/50], Train Loss: 2.0678, Val Loss: 2.0644, Val Acc: 39.44%\n","Epoch [4/50], Train Loss: 2.0602, Val Loss: 2.0644, Val Acc: 39.49%\n","Epoch [5/50], Train Loss: 2.0588, Val Loss: 2.0395, Val Acc: 42.02%\n","Epoch [6/50], Train Loss: 2.0534, Val Loss: 2.0416, Val Acc: 41.83%\n","Epoch [7/50], Train Loss: 2.0487, Val Loss: 2.0481, Val Acc: 41.11%\n","Epoch [8/50], Train Loss: 2.0513, Val Loss: 2.0466, Val Acc: 41.36%\n","Epoch [9/50], Train Loss: 2.0487, Val Loss: 2.0455, Val Acc: 41.44%\n","Epoch [10/50], Train Loss: 2.0472, Val Loss: 2.0469, Val Acc: 41.29%\n","Epoch [11/50], Train Loss: 2.0460, Val Loss: 2.0570, Val Acc: 40.31%\n","Epoch [12/50], Train Loss: 2.0459, Val Loss: 2.0451, Val Acc: 41.52%\n","Epoch [13/50], Train Loss: 2.0419, Val Loss: 2.0432, Val Acc: 41.67%\n","Epoch [14/50], Train Loss: 2.0416, Val Loss: 2.0435, Val Acc: 41.69%\n","Epoch [15/50], Train Loss: 2.0436, Val Loss: 2.0443, Val Acc: 41.60%\n","Epoch [16/50], Train Loss: 2.0332, Val Loss: 2.0399, Val Acc: 42.03%\n","Epoch [17/50], Train Loss: 2.0376, Val Loss: 2.0593, Val Acc: 40.08%\n","Epoch [18/50], Train Loss: 2.0438, Val Loss: 2.0590, Val Acc: 40.13%\n","Epoch [19/50], Train Loss: 2.0349, Val Loss: 2.0336, Val Acc: 42.66%\n","Epoch [20/50], Train Loss: 2.0348, Val Loss: 2.0458, Val Acc: 41.47%\n","Epoch [21/50], Train Loss: 2.0368, Val Loss: 2.0393, Val Acc: 42.13%\n","Epoch [22/50], Train Loss: 2.0355, Val Loss: 2.0349, Val Acc: 42.53%\n","Epoch [23/50], Train Loss: 2.0314, Val Loss: 2.0389, Val Acc: 42.17%\n","Epoch [24/50], Train Loss: 2.0279, Val Loss: 2.0413, Val Acc: 41.95%\n","Epoch [25/50], Train Loss: 2.0294, Val Loss: 2.0325, Val Acc: 42.77%\n","Epoch [26/50], Train Loss: 2.0205, Val Loss: 2.0248, Val Acc: 43.55%\n","Epoch [27/50], Train Loss: 2.0256, Val Loss: 2.0432, Val Acc: 41.76%\n","Epoch [28/50], Train Loss: 2.0230, Val Loss: 2.0277, Val Acc: 43.31%\n","Epoch [29/50], Train Loss: 2.0239, Val Loss: 2.0361, Val Acc: 42.48%\n","Epoch [30/50], Train Loss: 2.0245, Val Loss: 2.0365, Val Acc: 42.42%\n","Epoch [31/50], Train Loss: 2.0292, Val Loss: 2.0361, Val Acc: 42.46%\n","Epoch [32/50], Train Loss: 2.0310, Val Loss: 2.0327, Val Acc: 42.81%\n","Epoch [33/50], Train Loss: 2.0308, Val Loss: 2.0524, Val Acc: 40.83%\n","Epoch [34/50], Train Loss: 2.0250, Val Loss: 2.0238, Val Acc: 43.65%\n","Epoch [35/50], Train Loss: 2.0203, Val Loss: 2.0246, Val Acc: 43.58%\n","Epoch [36/50], Train Loss: 2.0299, Val Loss: 2.0383, Val Acc: 42.25%\n","Epoch [37/50], Train Loss: 2.0236, Val Loss: 2.0308, Val Acc: 43.00%\n","Epoch [38/50], Train Loss: 2.0178, Val Loss: 2.0201, Val Acc: 44.08%\n","Epoch [39/50], Train Loss: 2.0193, Val Loss: 2.0345, Val Acc: 42.61%\n","Epoch [40/50], Train Loss: 2.0259, Val Loss: 2.0333, Val Acc: 42.76%\n","Epoch [41/50], Train Loss: 2.0207, Val Loss: 2.0320, Val Acc: 42.85%\n","Epoch [42/50], Train Loss: 2.0218, Val Loss: 2.0213, Val Acc: 43.97%\n","Epoch [43/50], Train Loss: 2.0172, Val Loss: 2.0389, Val Acc: 42.17%\n","Epoch [44/50], Train Loss: 2.0219, Val Loss: 2.0390, Val Acc: 42.17%\n","Epoch [45/50], Train Loss: 2.0264, Val Loss: 2.0263, Val Acc: 43.50%\n","Epoch [46/50], Train Loss: 2.0186, Val Loss: 2.0249, Val Acc: 43.61%\n","Epoch [47/50], Train Loss: 2.0164, Val Loss: 2.0222, Val Acc: 43.90%\n","Epoch [48/50], Train Loss: 2.0119, Val Loss: 2.0311, Val Acc: 43.00%\n","Epoch [49/50], Train Loss: 2.0210, Val Loss: 2.0292, Val Acc: 43.15%\n","Epoch [50/50], Train Loss: 2.0213, Val Loss: 2.0286, Val Acc: 43.23%\n","Finished Training/Validation\n","Accuracy of the network on the 10000 test images: 43.23 %\n"]}]},{"cell_type":"code","source":["#SGD\n","\n","# Instanciar o modelo\n","model = SimpleMLP().to(device)\n","print(model)\n","\n","# Definir a função de perda\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","\n","\n","model_perform={}\n","#treina o modelo com 30 epocas\n","model_perform['train_losses'], model_perform['validation_losses'] = train(model, trainloader, testloader, criterion, optimizer, epochs=50)\n","model_perform['accuracy'] = test(model, testloader)\n","results_mlp['sgd'] = model_perform\n","torch.save(model.state_dict(),'sgd_mlp_cifar10')\n","\n"],"metadata":{"id":"I0oK_vwmlxIM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717467439554,"user_tz":180,"elapsed":823801,"user":{"displayName":"Henrique F","userId":"00712228040511115186"}},"outputId":"5b4eed62-3af5-49e2-dc94-80017924bc1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleMLP(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (fc1): Sequential(\n","    (0): Linear(in_features=3072, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.25, inplace=False)\n","  )\n","  (out): Sequential(\n","    (0): Linear(in_features=512, out_features=10, bias=True)\n","    (1): Softmax(dim=1)\n","  )\n",")\n","Epoch [1/50], Train Loss: 2.2993, Val Loss: 2.2965, Val Acc: 20.49%\n","Epoch [2/50], Train Loss: 2.2937, Val Loss: 2.2901, Val Acc: 22.61%\n","Epoch [3/50], Train Loss: 2.2869, Val Loss: 2.2823, Val Acc: 23.55%\n","Epoch [4/50], Train Loss: 2.2788, Val Loss: 2.2732, Val Acc: 23.41%\n","Epoch [5/50], Train Loss: 2.2698, Val Loss: 2.2635, Val Acc: 23.33%\n","Epoch [6/50], Train Loss: 2.2604, Val Loss: 2.2537, Val Acc: 23.43%\n","Epoch [7/50], Train Loss: 2.2509, Val Loss: 2.2443, Val Acc: 23.49%\n","Epoch [8/50], Train Loss: 2.2421, Val Loss: 2.2359, Val Acc: 23.80%\n","Epoch [9/50], Train Loss: 2.2346, Val Loss: 2.2287, Val Acc: 24.01%\n","Epoch [10/50], Train Loss: 2.2282, Val Loss: 2.2228, Val Acc: 24.31%\n","Epoch [11/50], Train Loss: 2.2228, Val Loss: 2.2177, Val Acc: 24.43%\n","Epoch [12/50], Train Loss: 2.2183, Val Loss: 2.2132, Val Acc: 24.64%\n","Epoch [13/50], Train Loss: 2.2140, Val Loss: 2.2092, Val Acc: 25.01%\n","Epoch [14/50], Train Loss: 2.2102, Val Loss: 2.2055, Val Acc: 25.35%\n","Epoch [15/50], Train Loss: 2.2065, Val Loss: 2.2020, Val Acc: 25.82%\n","Epoch [16/50], Train Loss: 2.2033, Val Loss: 2.1987, Val Acc: 26.40%\n","Epoch [17/50], Train Loss: 2.2001, Val Loss: 2.1954, Val Acc: 26.76%\n","Epoch [18/50], Train Loss: 2.1969, Val Loss: 2.1923, Val Acc: 27.22%\n","Epoch [19/50], Train Loss: 2.1938, Val Loss: 2.1892, Val Acc: 27.57%\n","Epoch [20/50], Train Loss: 2.1905, Val Loss: 2.1862, Val Acc: 28.00%\n","Epoch [21/50], Train Loss: 2.1877, Val Loss: 2.1833, Val Acc: 28.51%\n","Epoch [22/50], Train Loss: 2.1849, Val Loss: 2.1804, Val Acc: 28.91%\n","Epoch [23/50], Train Loss: 2.1820, Val Loss: 2.1776, Val Acc: 29.35%\n","Epoch [24/50], Train Loss: 2.1794, Val Loss: 2.1749, Val Acc: 29.72%\n","Epoch [25/50], Train Loss: 2.1767, Val Loss: 2.1721, Val Acc: 30.05%\n","Epoch [26/50], Train Loss: 2.1736, Val Loss: 2.1694, Val Acc: 30.44%\n","Epoch [27/50], Train Loss: 2.1709, Val Loss: 2.1667, Val Acc: 30.78%\n","Epoch [28/50], Train Loss: 2.1684, Val Loss: 2.1640, Val Acc: 31.04%\n","Epoch [29/50], Train Loss: 2.1656, Val Loss: 2.1613, Val Acc: 31.26%\n","Epoch [30/50], Train Loss: 2.1630, Val Loss: 2.1587, Val Acc: 31.77%\n","Epoch [31/50], Train Loss: 2.1607, Val Loss: 2.1562, Val Acc: 32.14%\n","Epoch [32/50], Train Loss: 2.1577, Val Loss: 2.1536, Val Acc: 32.43%\n","Epoch [33/50], Train Loss: 2.1552, Val Loss: 2.1512, Val Acc: 32.57%\n","Epoch [34/50], Train Loss: 2.1527, Val Loss: 2.1487, Val Acc: 32.78%\n","Epoch [35/50], Train Loss: 2.1506, Val Loss: 2.1462, Val Acc: 32.95%\n","Epoch [36/50], Train Loss: 2.1480, Val Loss: 2.1438, Val Acc: 33.27%\n","Epoch [37/50], Train Loss: 2.1456, Val Loss: 2.1414, Val Acc: 33.60%\n","Epoch [38/50], Train Loss: 2.1431, Val Loss: 2.1392, Val Acc: 33.85%\n","Epoch [39/50], Train Loss: 2.1408, Val Loss: 2.1370, Val Acc: 34.09%\n","Epoch [40/50], Train Loss: 2.1387, Val Loss: 2.1349, Val Acc: 34.30%\n","Epoch [41/50], Train Loss: 2.1364, Val Loss: 2.1329, Val Acc: 34.51%\n","Epoch [42/50], Train Loss: 2.1346, Val Loss: 2.1311, Val Acc: 34.62%\n","Epoch [43/50], Train Loss: 2.1328, Val Loss: 2.1293, Val Acc: 34.78%\n","Epoch [44/50], Train Loss: 2.1313, Val Loss: 2.1277, Val Acc: 34.88%\n","Epoch [45/50], Train Loss: 2.1295, Val Loss: 2.1260, Val Acc: 35.01%\n","Epoch [46/50], Train Loss: 2.1277, Val Loss: 2.1245, Val Acc: 35.10%\n","Epoch [47/50], Train Loss: 2.1264, Val Loss: 2.1230, Val Acc: 35.18%\n","Epoch [48/50], Train Loss: 2.1249, Val Loss: 2.1216, Val Acc: 35.30%\n","Epoch [49/50], Train Loss: 2.1235, Val Loss: 2.1202, Val Acc: 35.42%\n","Epoch [50/50], Train Loss: 2.1222, Val Loss: 2.1189, Val Acc: 35.59%\n","Finished Training/Validation\n","Accuracy of the network on the 10000 test images: 35.59 %\n"]}]},{"cell_type":"code","source":["def train_hess(model, trainloader, testloader, criterion, optimizer, epochs=10):\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(epochs):\n","        #training\n","        model.train()\n","        train_loss = 0.0\n","        for inputs, labels in trainloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward(create_graph=True)\n","            optimizer.step()\n","\n","\n","            train_loss += loss.item()*inputs.size(0)\n","        train_loss = train_loss/len(trainloader.dataset)\n","        train_losses.append(train_loss)\n","\n","        #validation\n","        model.eval()\n","        val_loss = 0.0\n","        correct = 0\n","        total = 0\n","        with torch.no_grad():\n","            for inputs, labels in testloader:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                # Forward pass\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate accuracy\n","                _, predicted = torch.max(outputs.data, 1)\n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","        val_loss = val_loss / len(testloader.dataset)\n","        val_losses.append(val_loss)\n","        print('Epoch [{}/{}], Train Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.2f}%'\n","              .format(epoch+1, epochs, train_loss, val_loss, 100 * correct / total))\n","\n","\n","\n","\n","\n","    print(\"Finished Training/Validation\")\n","    return train_losses.copy(), val_losses.copy()\n","\n","\n","\n","#ADAHESSIAN\n","\n","# Instanciar o modelo\n","model = SimpleMLP().to(device)\n","print(model)\n","\n","# Definir a função de perda\n","criterion = nn.CrossEntropyLoss()\n","optimizer = ada_optim.Adahessian(model.parameters(), lr = 0.01)\n","\n","\n","\n","model_perform={}\n","#treina o modelo com 30 epocas\n","model_perform['train_losses'], model_perform['validation_losses'] = train_hess(model, trainloader, testloader, criterion, optimizer, epochs=50)\n","model_perform['accuracy'] = test(model, testloader)\n","results_mlp['Adahessian'] = model_perform\n","torch.save(model.state_dict(),'adahessian_mlp_cifar10')"],"metadata":{"id":"3v18JfWMmBpu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717469502531,"user_tz":180,"elapsed":940439,"user":{"displayName":"Henrique F","userId":"00712228040511115186"}},"outputId":"cf905e64-e786-466c-b591-31a213ba0d44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleMLP(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (fc1): Sequential(\n","    (0): Linear(in_features=3072, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.25, inplace=False)\n","  )\n","  (out): Sequential(\n","    (0): Linear(in_features=512, out_features=10, bias=True)\n","    (1): Softmax(dim=1)\n","  )\n",")\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1203.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/50], Train Loss: 2.2687, Val Loss: 2.2330, Val Acc: 24.45%\n","Epoch [2/50], Train Loss: 2.2157, Val Loss: 2.1972, Val Acc: 28.11%\n","Epoch [3/50], Train Loss: 2.1875, Val Loss: 2.1729, Val Acc: 30.65%\n","Epoch [4/50], Train Loss: 2.1662, Val Loss: 2.1537, Val Acc: 32.57%\n","Epoch [5/50], Train Loss: 2.1487, Val Loss: 2.1379, Val Acc: 34.20%\n","Epoch [6/50], Train Loss: 2.1344, Val Loss: 2.1265, Val Acc: 34.97%\n","Epoch [7/50], Train Loss: 2.1249, Val Loss: 2.1181, Val Acc: 35.47%\n","Epoch [8/50], Train Loss: 2.1172, Val Loss: 2.1113, Val Acc: 35.97%\n","Epoch [9/50], Train Loss: 2.1112, Val Loss: 2.1054, Val Acc: 36.64%\n","Epoch [10/50], Train Loss: 2.1053, Val Loss: 2.1000, Val Acc: 37.20%\n","Epoch [11/50], Train Loss: 2.1002, Val Loss: 2.0953, Val Acc: 37.47%\n","Epoch [12/50], Train Loss: 2.0954, Val Loss: 2.0904, Val Acc: 37.95%\n","Epoch [13/50], Train Loss: 2.0906, Val Loss: 2.0860, Val Acc: 38.37%\n","Epoch [14/50], Train Loss: 2.0867, Val Loss: 2.0816, Val Acc: 38.80%\n","Epoch [15/50], Train Loss: 2.0819, Val Loss: 2.0776, Val Acc: 39.33%\n","Epoch [16/50], Train Loss: 2.0783, Val Loss: 2.0739, Val Acc: 39.90%\n","Epoch [17/50], Train Loss: 2.0744, Val Loss: 2.0705, Val Acc: 40.15%\n","Epoch [18/50], Train Loss: 2.0706, Val Loss: 2.0675, Val Acc: 40.55%\n","Epoch [19/50], Train Loss: 2.0675, Val Loss: 2.0647, Val Acc: 40.63%\n","Epoch [20/50], Train Loss: 2.0647, Val Loss: 2.0620, Val Acc: 40.85%\n","Epoch [21/50], Train Loss: 2.0612, Val Loss: 2.0595, Val Acc: 41.24%\n","Epoch [22/50], Train Loss: 2.0586, Val Loss: 2.0572, Val Acc: 41.34%\n","Epoch [23/50], Train Loss: 2.0560, Val Loss: 2.0549, Val Acc: 41.59%\n","Epoch [24/50], Train Loss: 2.0541, Val Loss: 2.0528, Val Acc: 41.74%\n","Epoch [25/50], Train Loss: 2.0512, Val Loss: 2.0507, Val Acc: 41.72%\n","Epoch [26/50], Train Loss: 2.0488, Val Loss: 2.0486, Val Acc: 41.96%\n","Epoch [27/50], Train Loss: 2.0467, Val Loss: 2.0470, Val Acc: 42.28%\n","Epoch [28/50], Train Loss: 2.0444, Val Loss: 2.0453, Val Acc: 42.37%\n","Epoch [29/50], Train Loss: 2.0420, Val Loss: 2.0433, Val Acc: 42.49%\n","Epoch [30/50], Train Loss: 2.0413, Val Loss: 2.0420, Val Acc: 42.55%\n","Epoch [31/50], Train Loss: 2.0388, Val Loss: 2.0405, Val Acc: 42.73%\n","Epoch [32/50], Train Loss: 2.0374, Val Loss: 2.0388, Val Acc: 42.86%\n","Epoch [33/50], Train Loss: 2.0348, Val Loss: 2.0374, Val Acc: 42.95%\n","Epoch [34/50], Train Loss: 2.0330, Val Loss: 2.0360, Val Acc: 43.08%\n","Epoch [35/50], Train Loss: 2.0312, Val Loss: 2.0346, Val Acc: 43.18%\n","Epoch [36/50], Train Loss: 2.0302, Val Loss: 2.0330, Val Acc: 43.36%\n","Epoch [37/50], Train Loss: 2.0283, Val Loss: 2.0321, Val Acc: 43.52%\n","Epoch [38/50], Train Loss: 2.0269, Val Loss: 2.0308, Val Acc: 43.68%\n","Epoch [39/50], Train Loss: 2.0253, Val Loss: 2.0296, Val Acc: 43.72%\n","Epoch [40/50], Train Loss: 2.0243, Val Loss: 2.0284, Val Acc: 43.80%\n","Epoch [41/50], Train Loss: 2.0230, Val Loss: 2.0273, Val Acc: 44.04%\n","Epoch [42/50], Train Loss: 2.0216, Val Loss: 2.0265, Val Acc: 44.09%\n","Epoch [43/50], Train Loss: 2.0202, Val Loss: 2.0251, Val Acc: 44.29%\n","Epoch [44/50], Train Loss: 2.0184, Val Loss: 2.0244, Val Acc: 44.35%\n","Epoch [45/50], Train Loss: 2.0172, Val Loss: 2.0231, Val Acc: 44.47%\n","Epoch [46/50], Train Loss: 2.0163, Val Loss: 2.0222, Val Acc: 44.44%\n","Epoch [47/50], Train Loss: 2.0152, Val Loss: 2.0212, Val Acc: 44.55%\n","Epoch [48/50], Train Loss: 2.0132, Val Loss: 2.0203, Val Acc: 44.58%\n","Epoch [49/50], Train Loss: 2.0122, Val Loss: 2.0193, Val Acc: 44.66%\n","Epoch [50/50], Train Loss: 2.0117, Val Loss: 2.0185, Val Acc: 44.82%\n","Finished Training/Validation\n","Accuracy of the network on the 10000 test images: 44.82 %\n"]}]},{"cell_type":"code","source":["import json\n","with open('results_mlp.json', 'w') as f:\n","    json.dump(results_mlp, f)\n","results_mlp"],"metadata":{"id":"C0Ds_QUYnPut","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717469622767,"user_tz":180,"elapsed":341,"user":{"displayName":"Henrique F","userId":"00712228040511115186"}},"outputId":"c1d9430e-729f-45a1-8956-7146cf8cd44b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'adam': {'train_losses': [2.1062370693588255,\n","   2.076851094207764,\n","   2.0678223641967772,\n","   2.0602485543060305,\n","   2.0587779837799074,\n","   2.0534436991119387,\n","   2.048690745010376,\n","   2.051316252593994,\n","   2.0486506270980835,\n","   2.0472399550628664,\n","   2.0459978631591795,\n","   2.0459018352508545,\n","   2.041879363288879,\n","   2.0415551443862916,\n","   2.043610722885132,\n","   2.0331813047790526,\n","   2.037590685119629,\n","   2.0437876927947998,\n","   2.0348826806640625,\n","   2.034775110015869,\n","   2.0367631427764894,\n","   2.035511237182617,\n","   2.0313923596191406,\n","   2.0278783959960935,\n","   2.0293517457580568,\n","   2.020473405380249,\n","   2.025565743789673,\n","   2.0230034784698487,\n","   2.0239455332946776,\n","   2.024515530090332,\n","   2.0291555434417723,\n","   2.0310216804504395,\n","   2.0307760675811766,\n","   2.024975893936157,\n","   2.020272402267456,\n","   2.029899219207764,\n","   2.023634532852173,\n","   2.0178457112121584,\n","   2.019310655899048,\n","   2.025860979309082,\n","   2.020700854034424,\n","   2.0217905261993407,\n","   2.017151405105591,\n","   2.021897798652649,\n","   2.026359996948242,\n","   2.0185883151245116,\n","   2.016411352996826,\n","   2.011942948150635,\n","   2.0209782969665526,\n","   2.0212545082092284],\n","  'validation_losses': [2.079146026611328,\n","   2.0557035049438475,\n","   2.0643684455871583,\n","   2.0644378791809084,\n","   2.0395024017333983,\n","   2.0416282131195067,\n","   2.0480891372680663,\n","   2.0466184009552,\n","   2.045479804611206,\n","   2.0469464305877687,\n","   2.0569580993652345,\n","   2.045077583694458,\n","   2.043188529586792,\n","   2.0435415790557863,\n","   2.0443421478271486,\n","   2.0398642822265627,\n","   2.0592744159698486,\n","   2.0590112075805664,\n","   2.033584801864624,\n","   2.045758653640747,\n","   2.039258277130127,\n","   2.0349249771118165,\n","   2.038924041748047,\n","   2.0413115631103516,\n","   2.032484694671631,\n","   2.024788162612915,\n","   2.0431693939208984,\n","   2.0277152687072753,\n","   2.0361120613098143,\n","   2.0364536293029785,\n","   2.0361298736572264,\n","   2.0327481147766115,\n","   2.052355308532715,\n","   2.0237932075500487,\n","   2.0246190490722658,\n","   2.0382857315063476,\n","   2.03079206237793,\n","   2.0200638942718507,\n","   2.0344758796691895,\n","   2.0333117828369143,\n","   2.0319746578216553,\n","   2.021318952178955,\n","   2.038873788833618,\n","   2.039012702560425,\n","   2.0263019027709963,\n","   2.024939740371704,\n","   2.0222125854492186,\n","   2.0310737071990967,\n","   2.0292048503875733,\n","   2.028600012969971],\n","  'accuracy': 0.4323},\n"," 'sgd': {'train_losses': [2.2993445458984376,\n","   2.293725809249878,\n","   2.286894242477417,\n","   2.2788011852264405,\n","   2.2698139207458494,\n","   2.2604337325286865,\n","   2.250862152862549,\n","   2.242096885986328,\n","   2.234618942642212,\n","   2.2281559091186525,\n","   2.2227592905426024,\n","   2.2183399530029297,\n","   2.2140302236938476,\n","   2.2101728727722167,\n","   2.2064720957946777,\n","   2.203338684082031,\n","   2.2000723791503907,\n","   2.1969233072662355,\n","   2.193756919174194,\n","   2.1905371600341796,\n","   2.187687317276001,\n","   2.1848812087249754,\n","   2.1820013105773928,\n","   2.1793567079925538,\n","   2.176688474731445,\n","   2.173603385925293,\n","   2.1709408060455324,\n","   2.1684345111083982,\n","   2.1656226647949217,\n","   2.1630337047576904,\n","   2.1607301804351806,\n","   2.1577413552856446,\n","   2.1552410231018064,\n","   2.152653209838867,\n","   2.1505686517333986,\n","   2.1479728463745116,\n","   2.145558114776611,\n","   2.1431486644744875,\n","   2.1408095513916017,\n","   2.1387371700286866,\n","   2.136448610458374,\n","   2.1345957443237307,\n","   2.1328412590789796,\n","   2.1312620122528076,\n","   2.1295470905303957,\n","   2.1276825923538207,\n","   2.126396496963501,\n","   2.1249300462341307,\n","   2.123547452697754,\n","   2.122187569732666],\n","  'validation_losses': [2.2964703907012938,\n","   2.2901296268463134,\n","   2.282309819793701,\n","   2.2731777141571046,\n","   2.263468129730225,\n","   2.2536765266418457,\n","   2.2442604095458982,\n","   2.2358549926757814,\n","   2.228744564819336,\n","   2.2227696853637697,\n","   2.2176961688995362,\n","   2.213249200820923,\n","   2.209230076599121,\n","   2.2055035854339597,\n","   2.2020064010620115,\n","   2.1986815170288088,\n","   2.1954449523925783,\n","   2.1922730827331542,\n","   2.189197788238525,\n","   2.1861870807647703,\n","   2.1832653469085694,\n","   2.180427251434326,\n","   2.1776235511779785,\n","   2.17486979598999,\n","   2.1721297008514404,\n","   2.1693810997009275,\n","   2.1666702491760255,\n","   2.163952733230591,\n","   2.161288960266113,\n","   2.158679839706421,\n","   2.1561538356781007,\n","   2.153649423980713,\n","   2.151162376403809,\n","   2.148708588409424,\n","   2.1462489334106447,\n","   2.143825549697876,\n","   2.141430954360962,\n","   2.139151892089844,\n","   2.1369753021240236,\n","   2.1349137046813964,\n","   2.1329448513031006,\n","   2.1310878032684326,\n","   2.1293270690917967,\n","   2.127651728820801,\n","   2.126003914642334,\n","   2.1244753978729247,\n","   2.1230052154541017,\n","   2.12159013710022,\n","   2.1202256256103515,\n","   2.118929664230347],\n","  'accuracy': 0.3559},\n"," 'Adahessian': {'train_losses': [2.2687360690307616,\n","   2.2157222807312014,\n","   2.1875321437072754,\n","   2.1661743785095213,\n","   2.1487233055114747,\n","   2.1343945206451416,\n","   2.1249182345581055,\n","   2.117180916595459,\n","   2.111244871673584,\n","   2.1052887061309815,\n","   2.1002409242248534,\n","   2.0954274321746826,\n","   2.090649072570801,\n","   2.0866707355499265,\n","   2.0818748880004883,\n","   2.0782684592437746,\n","   2.0743813780212403,\n","   2.0705571642303466,\n","   2.067482158164978,\n","   2.064731032485962,\n","   2.0611766654586794,\n","   2.0586051084899903,\n","   2.055994924926758,\n","   2.05411503993988,\n","   2.0511546091461184,\n","   2.0488327709198,\n","   2.0466903783416748,\n","   2.044358679046631,\n","   2.0419857370758057,\n","   2.041323362350464,\n","   2.0388176042556765,\n","   2.0373794816589355,\n","   2.0348252098846435,\n","   2.03302581867218,\n","   2.031216037902832,\n","   2.0301535999298097,\n","   2.0282525258636475,\n","   2.0269111102294923,\n","   2.0253119985198973,\n","   2.0242694467544555,\n","   2.0230102465057374,\n","   2.021644743232727,\n","   2.020158559761047,\n","   2.0183510664367676,\n","   2.017223179626465,\n","   2.016301995277405,\n","   2.015243416671753,\n","   2.0131730879974366,\n","   2.012205965118408,\n","   2.011665503082275],\n","  'validation_losses': [2.233011238861084,\n","   2.197158274459839,\n","   2.1728674919128417,\n","   2.1537069103240967,\n","   2.137880406951904,\n","   2.126517144012451,\n","   2.118055852508545,\n","   2.111278535079956,\n","   2.105360941696167,\n","   2.0999998859405515,\n","   2.0952766983032225,\n","   2.0903694194793703,\n","   2.0859630401611327,\n","   2.081645393371582,\n","   2.077577827835083,\n","   2.0739144886016847,\n","   2.0705476097106934,\n","   2.067503645324707,\n","   2.0646503746032714,\n","   2.0619665699005125,\n","   2.05948576965332,\n","   2.0571633125305175,\n","   2.054875415420532,\n","   2.0527782012939455,\n","   2.050655595397949,\n","   2.0486444526672365,\n","   2.046957076263428,\n","   2.045291314315796,\n","   2.0432758598327636,\n","   2.041992206954956,\n","   2.0404849075317384,\n","   2.038819861602783,\n","   2.037409383010864,\n","   2.0360459545135496,\n","   2.0346118713378907,\n","   2.033006227874756,\n","   2.0321164375305174,\n","   2.0307921188354494,\n","   2.029587587356567,\n","   2.0283975498199465,\n","   2.027336111068726,\n","   2.026456562805176,\n","   2.02509814453125,\n","   2.024448426437378,\n","   2.023123429489136,\n","   2.022243114089966,\n","   2.0211672359466553,\n","   2.020318851470947,\n","   2.019270693206787,\n","   2.0185389305114745],\n","  'accuracy': 0.4482}}"]},"metadata":{},"execution_count":18}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}